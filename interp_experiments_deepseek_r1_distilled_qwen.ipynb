{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not installed, using eager implementation of SAE decoder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "sys.path.append('sparsify')\n",
    "\n",
    "from sparsify import Sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x27150825e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_device(DEVICE)\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Recommendations\n",
    "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B#usage-recommendations\n",
    "\n",
    "\n",
    "1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "\n",
    "2. Avoid adding a system prompt; all instructions should be contained within the user prompt.\n",
    "\n",
    "3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "\n",
    "4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "\n",
    "Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')\n",
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(chat_prompt, max_new_tokens=400, temperature=0.6, add_generation_prompt=True, output_hidden_states=True):\n",
    "    tokenized_inputs = tokenizer.apply_chat_template(chat_prompt, tokenize=True, add_generation_prompt=add_generation_prompt, return_tensors='pt', return_dict=True)\n",
    "\n",
    "    model_out = model.generate(\n",
    "        tokenized_inputs.input_ids,\n",
    "        attention_mask=tokenized_inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_inference(model_out_sequences, skip_special_tokens=False):\n",
    "    decoded_out = tokenizer.batch_decode(model_out_sequences, skip_special_tokens=skip_special_tokens)\n",
    "    return decoded_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NUM_STR = '01'\n",
    "\n",
    "EXPERIMENT_FOLDER_NAME = os.path.join('experiments_files', f'experiment_{EXPERIMENT_NUM_STR}')\n",
    "\n",
    "EXPERIMENT_REASONING_PATH = os.path.join(EXPERIMENT_FOLDER_NAME, 'reasoning')\n",
    "\n",
    "MODEL_OUT_CACHE_FOLDER_PATH = os.path.join(EXPERIMENT_REASONING_PATH, 'model_outputs_cache')\n",
    "\n",
    "PLOTS_PATH = os.path.join(EXPERIMENT_REASONING_PATH, 'plots')\n",
    "ALL_TOKENS_PLOTS_PATH = os.path.join(PLOTS_PATH, 'all_tokens')\n",
    "GENERATED_ONLY_TOKENS_PLOTS_PATH = os.path.join(PLOTS_PATH, 'generated_tokens_only')\n",
    "\n",
    "paths_to_create = [\n",
    "    MODEL_OUT_CACHE_FOLDER_PATH,\n",
    "    ALL_TOKENS_PLOTS_PATH,\n",
    "    GENERATED_ONLY_TOKENS_PLOTS_PATH\n",
    "]\n",
    "\n",
    "for path in paths_to_create:\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_output(model_out, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    data_to_save = {\n",
    "        'sequences': model_out.sequences.cpu(),\n",
    "        'hidden_states': tuple(\n",
    "            tuple(state.cpu() for state in layer) for layer in model_out.hidden_states\n",
    "        )\n",
    "    }\n",
    "    torch.save(data_to_save, save_path)\n",
    "\n",
    "def load_model_output(load_path, device=DEVICE):\n",
    "    loaded_data = torch.load(load_path, map_location=device)\n",
    "    sequences = loaded_data['sequences']\n",
    "    hidden_states = tuple(\n",
    "        tuple(state.to(device) for state in layer) for layer in loaded_data['hidden_states']\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        sequences=sequences,\n",
    "        hidden_states=hidden_states\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_prompts(prompts, suffix=''):\n",
    "    chat_prompts = [\n",
    "        [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt_content + suffix\n",
    "            }\n",
    "        ] for prompt_content in prompts\n",
    "    ]\n",
    "    return chat_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_SUFFIX = ' Please reason step by step in a few words, and put your final answer within \\\\boxed{}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_processed_prompts(questions=None, answers=None, filename='processed_prompts.csv', suffix=''):\n",
    "#     if os.path.exists(filename):\n",
    "#         print(f'Loading from existing file: {filename}')\n",
    "#         return pd.read_csv(filename)\n",
    "    \n",
    "#     if questions is None or answers is None:\n",
    "#         print(f'No questions nor answers were provided and file {filename} wasn\\'t found')\n",
    "\n",
    "#     chat_prompts = create_chat_prompts(questions, suffix)\n",
    "#     prompts_df = pd.DataFrame({\n",
    "#         'chat_prompt': [str(prompt) for prompt in chat_prompts],\n",
    "#         'correct_answer': answers\n",
    "#     })\n",
    "#     prompts_df.to_csv(filename, index=False)\n",
    "#     print(f'Prompts file created: {filename}')\n",
    "#     return prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESONING_PROMPTS_FILENAME = 'reasoning_prompts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(RESONING_PROMPTS_FILENAME):\n",
    "#     gsm8k_ds = load_dataset('openai/gsm8k', 'main')\n",
    "\n",
    "#     reasoning_prompts = get_processed_prompts(\n",
    "#         questions=gsm8k_ds['train']['question'],\n",
    "#         answers=gsm8k_ds['train']['answer'],\n",
    "#         suffix=REASONING_SUFFIX,\n",
    "#         filename=RESONING_PROMPTS_FILENAME\n",
    "#     )\n",
    "\n",
    "#     del gsm8k_ds\n",
    "\n",
    "# reasoning_prompts = get_processed_prompts(filename=RESONING_PROMPTS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_ds = load_dataset('openai/gsm8k', 'main')\n",
    "\n",
    "reasoning_chat_prompts = create_chat_prompts(gsm8k_ds['train']['question'], suffix=REASONING_SUFFIX)\n",
    "\n",
    "del gsm8k_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for individual SAE encoder out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_encoder_out_by_acts(encoder_out, top_k=None, no_feature_repetition=False):\n",
    "    top_acts_flat = encoder_out.top_acts.flatten()\n",
    "    top_indices_flat = encoder_out.top_indices.flatten()\n",
    "\n",
    "    top_acts_num = top_acts_flat.size(0)\n",
    "    if top_k is None or top_k > top_acts_num:\n",
    "        top_k = top_acts_num\n",
    "\n",
    "    if no_feature_repetition:\n",
    "        feature_max_act = {}\n",
    "        for feature_idx_tn, act_val in zip(top_indices_flat, top_acts_flat):\n",
    "            feature_idx = feature_idx_tn.item()\n",
    "            if feature_idx not in feature_max_act or act_val > feature_max_act[feature_idx]:\n",
    "                feature_max_act[feature_idx] = act_val\n",
    "        \n",
    "        sorted_features = sorted(feature_max_act.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_k = min(top_k, len(sorted_features))\n",
    "        top_features = sorted_features[:top_k]\n",
    "        \n",
    "        unique_indices = torch.tensor([feature[0] for feature in top_features], device=top_indices_flat.device)\n",
    "        unique_acts = torch.tensor([feature[1] for feature in top_features], device=top_acts_flat.device)\n",
    "        \n",
    "        return SimpleNamespace(\n",
    "            top_acts=unique_acts,\n",
    "            top_indices=unique_indices\n",
    "        )\n",
    "\n",
    "    top_acts_values, top_flat_acts_pos = torch.topk(top_acts_flat, top_k)\n",
    "    corresponding_feature_indices = top_indices_flat[top_flat_acts_pos]\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        top_acts=top_acts_values,\n",
    "        top_indices=corresponding_feature_indices\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_feature_freq_encoder_out(all_steps_feature_indices, top_k=None):\n",
    "    flat_freq = torch.bincount(all_steps_feature_indices.flatten())\n",
    "    feature_indices_with_counts = torch.nonzero(flat_freq, as_tuple=False).squeeze()\n",
    "\n",
    "    feature_freqs = flat_freq[feature_indices_with_counts]\n",
    "    \n",
    "    features_num = feature_indices_with_counts.size(0)\n",
    "    if top_k is None or top_k > features_num:\n",
    "        top_k = features_num\n",
    "\n",
    "    top_k_freqs, top_k_indices = torch.topk(feature_freqs, top_k)\n",
    "    top_k_features = feature_indices_with_counts[top_k_indices]\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        feature_indices=top_k_features,\n",
    "        freqs=top_k_freqs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relate_feature_acts_to_tokens(encoder_out, feature_idx, sample_tokens, top_k=None, no_token_repetition=False):\n",
    "    feature_acts = torch.where(\n",
    "        encoder_out.top_indices == feature_idx,\n",
    "        encoder_out.top_acts,\n",
    "        torch.tensor(0.0, device=encoder_out.top_acts.device)\n",
    "    )\n",
    "    max_acts_per_token = feature_acts.max(dim=1).values\n",
    "    \n",
    "    # Not that necessary but just in case\n",
    "    nonzero_max_acts_mask = max_acts_per_token > 0\n",
    "    if nonzero_max_acts_mask.sum() == 0:\n",
    "        print(f'Non-dead feature activations weren\\'t found for feature with index {feature_idx}')\n",
    "        return torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    nonzero_max_acts = max_acts_per_token[nonzero_max_acts_mask]\n",
    "    nonzero_max_acts_indices = nonzero_max_acts_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if no_token_repetition:\n",
    "        nonzero_tokens = torch.tensor([sample_tokens[i].item() for i in nonzero_max_acts_indices], device=encoder_out.top_acts.device)\n",
    "        unique_tokens, inverse_indices = torch.unique(nonzero_tokens, return_inverse=True)\n",
    "        \n",
    "        unique_tokens_len = len(unique_tokens)\n",
    "        max_acts_per_unique_token = torch.zeros(unique_tokens_len, device=encoder_out.top_acts.device)\n",
    "        for i, token_idx in enumerate(inverse_indices):\n",
    "            max_acts_per_unique_token[token_idx] = max(max_acts_per_unique_token[token_idx], nonzero_max_acts[i])\n",
    "\n",
    "        if top_k is None or top_k > unique_tokens_len:\n",
    "            top_k = unique_tokens_len\n",
    "\n",
    "        top_k_acts, top_k_indices = torch.topk(max_acts_per_unique_token, top_k)\n",
    "        top_k_tokens = unique_tokens[top_k_indices]\n",
    "    else:\n",
    "        non_zero_acts_num = len(nonzero_max_acts)\n",
    "        if top_k is None or top_k > non_zero_acts_num:\n",
    "            top_k = non_zero_acts_num\n",
    "        \n",
    "        top_k_acts, top_k_indices = torch.topk(nonzero_max_acts, top_k)\n",
    "        top_k_tokens = torch.tensor([sample_tokens[i].item() for i in nonzero_max_acts_indices[top_k_indices]], device=encoder_out.top_acts.device)\n",
    "    \n",
    "    return SimpleNamespace(\n",
    "        token_ids=top_k_tokens,\n",
    "        top_acts=top_k_acts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for plotting individual encoder out analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_bar(df, x, y, title, labels, save_path=None):\n",
    "    bar_fig = px.bar(\n",
    "        df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        title=title,\n",
    "        labels=labels\n",
    "    )\n",
    "    bar_fig.update_layout(\n",
    "        xaxis={\n",
    "            'tickangle': -90,\n",
    "        },\n",
    "        yaxis={\n",
    "            'type': 'log'\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        # bar_fig.write_image(save_path)\n",
    "        bar_fig.write_html(save_path)\n",
    "        return\n",
    "    bar_fig.show()\n",
    "\n",
    "\n",
    "def plot_feature_acts(encoder_out, title, save_path=None):\n",
    "    all_outs_encoder_out_df = pd.DataFrame({\n",
    "        'index': encoder_out.top_indices.cpu().numpy().flatten(),\n",
    "        'act': encoder_out.top_acts.cpu().numpy().flatten()\n",
    "    })\n",
    "    all_outs_encoder_out_df['index'] = all_outs_encoder_out_df['index'].astype(str)\n",
    "    plot_simple_bar(\n",
    "        all_outs_encoder_out_df,\n",
    "        x='index',\n",
    "        y='act',\n",
    "        title=title,\n",
    "        labels={'index': 'Feature Index', 'act': 'Activation'},\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_feature_freqs(feature_freqs, title, save_path=None):\n",
    "    all_outs_encoder_out_df = pd.DataFrame({\n",
    "        'index': feature_freqs.feature_indices.cpu().numpy().flatten(),\n",
    "        'freq': feature_freqs.freqs.cpu().numpy().flatten()\n",
    "    })\n",
    "    all_outs_encoder_out_df['index'] = all_outs_encoder_out_df['index'].astype(str)\n",
    "    plot_simple_bar(\n",
    "        all_outs_encoder_out_df,\n",
    "        x='index',\n",
    "        y='freq',\n",
    "        title=title,\n",
    "        labels={'index': 'Feature Index', 'freq': 'Frequency'},\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_acts_histogram(encoder_out, title, nbins=50, save_path=None):\n",
    "    acts_hist_fig = px.histogram(\n",
    "        x=encoder_out.top_acts.cpu().numpy().flatten(),\n",
    "        nbins=nbins,\n",
    "        title=title,\n",
    "        labels={'x': 'Activation', 'y': ' Frequency'},\n",
    "    )\n",
    "    acts_hist_fig.update_yaxes(type='log')\n",
    "\n",
    "    if save_path:\n",
    "        # acts_hist_fig.write_image(save_path)\n",
    "        acts_hist_fig.write_html(save_path)\n",
    "        return\n",
    "    \n",
    "    acts_hist_fig.show()\n",
    "\n",
    "\n",
    "def plot_acts_and_tokens_relation_scatter(token_acts_relation, title, save_path=None):\n",
    "    token_act_df = pd.DataFrame({\n",
    "        'token_id': token_acts_relation.token_ids.cpu().numpy(),\n",
    "        'top_acts': token_acts_relation.top_acts.cpu().numpy()\n",
    "    })\n",
    "    scatter_fig = px.scatter(\n",
    "        token_act_df,\n",
    "        x='token_id',\n",
    "        y='top_acts',\n",
    "        title=title,\n",
    "        labels={'top_acts': 'Top Activations', 'token_id': 'Token ID'}\n",
    "    )\n",
    "    scatter_fig.update_traces(marker=dict(size=12, opacity=0.7))\n",
    "\n",
    "    if save_path:\n",
    "        # scatter_fig.write_image(save_path)\n",
    "        scatter_fig.write_html(save_path)\n",
    "        return\n",
    "    \n",
    "    scatter_fig.show()\n",
    "\n",
    "def plot_acts_and_tokens_relation_bar(token_acts_relation, title, save_path=None):\n",
    "    decoded_tokens = decode_inference(token_acts_relation.token_ids)\n",
    "    decoded_tokens = [\n",
    "        f'<space>:{token_id}' if token == ' ' else\n",
    "        f'<colon>:{token_id}' if token == ':' else\n",
    "        f'{token}:{token_id}'\n",
    "        for token, token_id in zip(decoded_tokens, token_acts_relation.token_ids.tolist())\n",
    "    ]\n",
    "    token_act_df = pd.DataFrame({\n",
    "        'token': decoded_tokens,\n",
    "        'top_acts': token_acts_relation.top_acts.cpu().numpy()\n",
    "    })\n",
    "\n",
    "    plot_height = max(400, len(token_act_df) * 15)\n",
    "    acts_tokens_bar_fig = px.bar(\n",
    "        token_act_df,\n",
    "        x='top_acts',\n",
    "        y='token',\n",
    "        orientation='h',\n",
    "        title=title,\n",
    "        labels={'top_acts': 'Top Activation', 'token': 'Token'},\n",
    "    )\n",
    "    acts_tokens_bar_fig.update_layout(\n",
    "        yaxis={\n",
    "            'categoryorder': 'total ascending',\n",
    "            'tickmode': 'array',\n",
    "            'tickvals': token_act_df['token'],\n",
    "            'ticktext': token_act_df['token']\n",
    "        },\n",
    "        xaxis={\n",
    "            'type': 'log'\n",
    "        },\n",
    "        margin=dict(l=200),\n",
    "        font=dict(size=10),\n",
    "        height=plot_height\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        # acts_tokens_bar_fig.write_image(save_path)\n",
    "        acts_tokens_bar_fig.write_html(save_path)\n",
    "        return\n",
    "    \n",
    "    acts_tokens_bar_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state_by_layer_acts(model_out, model_sae_layers):\n",
    "    hidden_state_by_layer = {}\n",
    "    generated_tokens_only_hidden_state_by_layer = {}\n",
    "\n",
    "    for layer in model_sae_layers:\n",
    "        model_sae_layer_acts = []\n",
    "        for step_idx in range(len(model_out.hidden_states)):\n",
    "            step_layer_acts = model_out.hidden_states[step_idx][layer]\n",
    "            # Remove batch dim\n",
    "            flattened_acts = step_layer_acts.reshape(-1, step_layer_acts.size(-1))\n",
    "            model_sae_layer_acts.append(flattened_acts)\n",
    "\n",
    "        hidden_state_by_layer[layer] = torch.cat(model_sae_layer_acts, dim=0)\n",
    "        generated_tokens_only_hidden_state_by_layer[layer] = torch.cat(model_sae_layer_acts[1:], dim=0)\n",
    "\n",
    "    return hidden_state_by_layer, generated_tokens_only_hidden_state_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_hidden_states_by_layer(hidden_state_by_layer, saes):\n",
    "    encoder_out_by_layer = {}\n",
    "    for layer_num in hidden_state_by_layer:\n",
    "        encoder_out_by_layer[layer_num] = saes[layer_num].encode(\n",
    "            hidden_state_by_layer[layer_num]\n",
    "        )\n",
    "    return encoder_out_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_enocder_out_by_layer(\n",
    "    encoder_out_by_layer,\n",
    "    top_k_encoder_out_by_acts,\n",
    "    top_k_encoder_out_feature_freq,\n",
    "    top_k_relevant_features,\n",
    "    token_acts_relation_top_k, # Not used in scatter plot\n",
    "    sample_token_ids,\n",
    "    plot_save_path,\n",
    "    plot_title_prefix_init=''\n",
    "):\n",
    "    for layer_num, encoder_out in encoder_out_by_layer.items():\n",
    "        top_k_encoder_out = get_top_k_encoder_out_by_acts(encoder_out, top_k=top_k_encoder_out_by_acts)\n",
    "        top_k_feature_freq = get_top_k_feature_freq_encoder_out(encoder_out.top_indices, top_k=top_k_encoder_out_feature_freq)\n",
    "        relevant_features = get_top_k_encoder_out_by_acts(encoder_out, top_k=top_k_relevant_features, no_feature_repetition=True).top_indices\n",
    "\n",
    "        # Create Plots\n",
    "        plot_title_prefix = f'Layer {layer_num} |'\n",
    "        if len(plot_title_prefix_init) > 0:\n",
    "            plot_title_prefix = plot_title_prefix_init + plot_title_prefix\n",
    "        \n",
    "        filename_prefix = f'layer_{layer_num}'\n",
    "\n",
    "        plot_feature_acts(\n",
    "            top_k_encoder_out,\n",
    "            title=f'{plot_title_prefix} Top {top_k_encoder_out_by_acts} Feature Activations',\n",
    "            save_path=os.path.join(plot_save_path, f'{filename_prefix}_feature_activations.html')\n",
    "        )\n",
    "        plot_feature_freqs(\n",
    "            top_k_feature_freq,\n",
    "            title=f'{plot_title_prefix} Top {top_k_encoder_out_feature_freq} Feature Frequencies',\n",
    "            save_path=os.path.join(plot_save_path, f'{filename_prefix}_feature_frequencies.html')\n",
    "        )\n",
    "        plot_acts_histogram(\n",
    "            encoder_out,\n",
    "            title=f'{plot_title_prefix} Activations Frequency',\n",
    "            save_path=os.path.join(plot_save_path, f'{filename_prefix}_activations_frequencies_hist.html')\n",
    "        )\n",
    "\n",
    "        for relevant_feature in relevant_features:\n",
    "            relevant_feature_idx = relevant_feature.item()\n",
    "            token_acts_relation = relate_feature_acts_to_tokens(\n",
    "                encoder_out,\n",
    "                feature_idx=relevant_feature_idx,\n",
    "                sample_tokens=sample_token_ids,\n",
    "                no_token_repetition=True\n",
    "            )\n",
    "            top_k_token_acts_relation = relate_feature_acts_to_tokens(\n",
    "                encoder_out,\n",
    "                feature_idx=relevant_feature_idx,\n",
    "                sample_tokens=sample_token_ids,\n",
    "                top_k=token_acts_relation_top_k,\n",
    "                no_token_repetition=True\n",
    "            )\n",
    "\n",
    "            plot_acts_and_tokens_relation_scatter(\n",
    "                token_acts_relation,\n",
    "                title=f'{plot_title_prefix} Top {token_acts_relation_top_k} Activating Tokens for Feature with index {relevant_feature_idx}',\n",
    "                save_path=os.path.join(plot_save_path, f'{filename_prefix}_feature_{relevant_feature_idx}_activations_tokens_scatter.html')\n",
    "            )\n",
    "            plot_acts_and_tokens_relation_bar(\n",
    "                top_k_token_acts_relation,\n",
    "                title=f'{plot_title_prefix} Top {token_acts_relation_top_k} Activating Tokens for Feature with index {relevant_feature_idx}',\n",
    "                save_path=os.path.join(plot_save_path, f'{filename_prefix}_feature_{relevant_feature_idx}_activations_tokens_bar.html')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_model_inferences(prompts, model_out_folder_path):\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        model_out = inference(prompt)\n",
    "        save_path = os.path.join(model_out_folder_path, f'{idx}_model_inference.pt')\n",
    "        save_model_output(model_out, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_model_inferences(saes, prompts, model_out_folder_path):\n",
    "    model_out_files = os.listdir(model_out_folder_path)\n",
    "    model_out_files.sort(key=lambda x: int(re.search(r'(\\d+)_model_inference.pt', x).group(1)))\n",
    "\n",
    "    # Top k Definitions Here\n",
    "    top_k_encoder_out_by_acts=400\n",
    "    top_k_encoder_out_feature_freq=70\n",
    "    top_k_relevant_features=10\n",
    "    token_acts_relation_top_k=70\n",
    "\n",
    "    for out_file_idx, out_file in enumerate(model_out_files):\n",
    "        model_out = load_model_output(\n",
    "            os.path.join(model_out_folder_path, out_file),\n",
    "            device=DEVICE\n",
    "        )\n",
    "        hidden_state_by_layer, generated_tokens_only_hidden_state_by_layer = get_hidden_state_by_layer_acts(model_out, saes.keys())\n",
    "\n",
    "        # Analyze all tokens\n",
    "        encoder_out_by_layer = encode_hidden_states_by_layer(hidden_state_by_layer, saes)\n",
    "\n",
    "        all_tokens_plots_by_model_out_path = os.path.join(ALL_TOKENS_PLOTS_PATH, f'{out_file_idx}_model_out')\n",
    "        os.makedirs(all_tokens_plots_by_model_out_path, exist_ok=True)\n",
    "\n",
    "        analyze_enocder_out_by_layer(\n",
    "            encoder_out_by_layer=encoder_out_by_layer,\n",
    "            top_k_encoder_out_by_acts=top_k_encoder_out_by_acts,\n",
    "            top_k_encoder_out_feature_freq=top_k_encoder_out_feature_freq,\n",
    "            top_k_relevant_features=top_k_relevant_features,\n",
    "            token_acts_relation_top_k=token_acts_relation_top_k,\n",
    "            sample_token_ids=model_out.sequences.squeeze(),\n",
    "            plot_save_path=all_tokens_plots_by_model_out_path,\n",
    "            plot_title_prefix_init=f'Model Out #{out_file_idx} | '\n",
    "        )\n",
    "\n",
    "        # Analyze generated tokens only\n",
    "        # The code here can be improved later\n",
    "        model_input_prompt_tokens = tokenizer.apply_chat_template(\n",
    "            prompts[out_file_idx],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt',\n",
    "            return_dict=True\n",
    "        ).input_ids\n",
    "        input_prompt_tokens_len = model_input_prompt_tokens.size()[1]\n",
    "        model_out_generated_only_token_ids = model_out.sequences.squeeze()[input_prompt_tokens_len:]\n",
    "\n",
    "        encoder_out_by_layer = encode_hidden_states_by_layer(generated_tokens_only_hidden_state_by_layer, saes)\n",
    "\n",
    "        generated_only_tokens_plots_by_model_out_path = os.path.join(GENERATED_ONLY_TOKENS_PLOTS_PATH, f'{out_file_idx}_model_out')\n",
    "        os.makedirs(generated_only_tokens_plots_by_model_out_path, exist_ok=True)\n",
    "\n",
    "        analyze_enocder_out_by_layer(\n",
    "            encoder_out_by_layer=encoder_out_by_layer,\n",
    "            top_k_encoder_out_by_acts=top_k_encoder_out_by_acts,\n",
    "            top_k_encoder_out_feature_freq=top_k_encoder_out_feature_freq,\n",
    "            top_k_relevant_features=top_k_relevant_features,\n",
    "            token_acts_relation_top_k=token_acts_relation_top_k,\n",
    "            sample_token_ids=model_out_generated_only_token_ids,\n",
    "            plot_save_path=generated_only_tokens_plots_by_model_out_path,\n",
    "            plot_title_prefix_init=f'Model Out #{out_file_idx} (Gen. Tokens Only) | '\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9ee60591724b419b692a6ee88bd4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0f5ae7e2d440ce88d88010eba50738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb78bf357642878a03797b9f183fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_sae_layers = [5, 10, 20]\n",
    "\n",
    "saes = {}\n",
    "for layer_num in model_sae_layers:\n",
    "    saes[layer_num] = Sae.load_from_hub('EleutherAI/sae-DeepSeek-R1-Distill-Qwen-1.5B-65k', hookpoint=f'layers.{layer_num}.mlp', device=DEVICE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_and_save_model_inferences(\n",
    "#     prompts=reasoning_chat_prompts,\n",
    "#     model_out_folder_path=MODEL_OUT_CACHE_FOLDER_PATH\n",
    "# )\n",
    "\n",
    "# -> Finished 31 inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_analyze_model_inferences(\n",
    "    saes=saes,\n",
    "    prompts=reasoning_chat_prompts,\n",
    "    model_out_folder_path=MODEL_OUT_CACHE_FOLDER_PATH\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
